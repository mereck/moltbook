services:
  # ── Firewall gateway ──────────────────────────────────────────
  # The agent shares this container's network namespace, so all
  # iptables OUTPUT rules here control the agent's egress too.
  # Default policy is DROP from the instant the container starts;
  # allow rules are then added for moltbook.com and ollama only.
  firewall:
    build: ./firewall
    container_name: llm-lockdown-firewall
    read_only: true
    cap_add:
      - NET_ADMIN
    cap_drop:
      - ALL
    security_opt:
      - no-new-privileges:true
    networks:
      sandbox_net:
    volumes:
      - ./firewall/allowed_hosts.txt:/etc/firewall/allowed_hosts.txt:ro
    healthcheck:
      test: ["CMD-SHELL", "iptables -L OUTPUT -n | grep -q DROP"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 3s
    restart: unless-stopped

  # ── LLM server (Ollama) ─────────────────────────────────────
  # Default model: qwen2.5:3b (good at structured JSON output).
  # Alternatives: use Docker Model Runner for Mac GPU acceleration,
  # vLLM for production GPU serving, or any OpenAI-compatible endpoint.
  # NOTE: ollama has internet access on sandbox_net because it
  # needs to pull the model on first boot. After the first run the
  # model is cached in the ollama_data volume. The agent cannot
  # abuse ollama's internet access — the agent's own egress is
  # locked down by the firewall's iptables OUTPUT chain.
  ollama:
    image: ollama/ollama:0.15.4
    container_name: llm-lockdown-ollama
    networks:
      sandbox_net:
    volumes:
      - ollama_data:/root/.ollama
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,nodev,size=256m
    mem_limit: 6g
    cpus: "2.0"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 10s

  # ── Model puller (runs once, then exits) ──────────────────────
  ollama-init:
    image: ollama/ollama:0.15.4
    container_name: llm-lockdown-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    networks:
      sandbox_net:
    environment:
      - OLLAMA_HOST=http://ollama:11434
    entrypoint: ["sh", "-c", "ollama pull ${OLLAMA_MODEL:-qwen2.5:3b}"]
    restart: "no"

  # ── Sandboxed agent ───────────────────────────────────────────
  agent:
    build: .
    container_name: llm-lockdown-agent
    depends_on:
      firewall:
        condition: service_healthy

    # Share the firewall's network — iptables rules apply to agent
    network_mode: "service:firewall"

    # --- Resource limits ---
    mem_limit: 256m
    memswap_limit: 256m
    cpus: "0.5"
    pids_limit: 64

    # --- Security hardening ---
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp=seccomp.json
    cap_drop:
      - ALL

    # --- Writable tmpfs only ---
    tmpfs:
      - /tmp:noexec,nosuid,nodev,size=32m

    # --- Config volume ---
    volumes:
      - ./agent_config.json:/etc/agent/config.json:ro

    # --- Environment ---
    environment:
      - PYTHONDONTWRITEBYTECODE=1
      - PYTHONUNBUFFERED=1
      - MOLTBOOK_API_KEY=${MOLTBOOK_API_KEY}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-qwen2.5:3b}
      - LLM_URL=${LLM_URL:-http://ollama:11434/v1/chat/completions}
      - AGENT_CONFIG=/etc/agent/config.json

    restart: unless-stopped

volumes:
  ollama_data:

networks:
  sandbox_net:
    driver: bridge
